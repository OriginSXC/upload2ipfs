name: IPFS Gateway Remote Probe (multi-region) + Patch index.html

on:
  workflow_dispatch:
    inputs:
      nodes:
        description: "Check-Host nodes per URL (3-7 recommended)"
        required: false
        default: "5"
      cid:
        description: "IPFS CID to test (only for probing; index.html 不改CID)"
        required: false
        default: "bafybeifx7yeb55armcsxwwitkymga5xf53dxiarykms3ygqic223w5sk3m"
  push:
    branches: [ "master", "main" ]
    paths:
      - 'index.html'
      - 'gateways.js'
      - 'gateways.txt'
      - '.github/workflows/ipfs-gateway-remote-probe.yml'
  schedule:
    - cron: "23 2 * * *"  # 每天 02:23 UTC 运行

permissions:
  contents: write

concurrency:
  group: ipfs-gateway-remote-probe
  cancel-in-progress: false

jobs:
  probe:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Python deps
        run: pip install requests

      - name: Write remote_probe_checkhost.py (URL parsing fixed)
        shell: bash
        run: |
          python - <<'PY'
          import textwrap, pathlib, re
          code = r'''
          #!/usr/bin/env python3
          # -*- coding: utf-8 -*-
          import argparse, csv, re, time, requests
          CHECK_HTTP = "https://check-host.net/check-http"
          CHECK_RESULT = "https://check-host.net/check-result/{rid}"
          DEFAULT_CID = "bafybeifx7yeb55armcsxwwitkymga5xf53dxiarykms3ygqic223w5sk3m"

          def parse_urls(path: str):
              url_regex = re.compile(r'https?://[^\s"\'`]+')
              found_urls = []
              with open(path, "r", encoding="utf-8", errors="ignore") as f:
                  text = f.read()
                  found_urls = url_regex.findall(text)
              clean, seen = [], set()
              for u in found_urls:
                  u = u.strip().rstrip(',').rstrip(']').rstrip(';')
                  if u and u not in seen:
                      seen.add(u)
                      clean.append(u)
              if not clean: raise RuntimeError(f"No http(s) URLs found in input file: {path}")
              return clean

          def build_test_url(base: str, cid: str):
              return ("POST", f"{base}{cid}") if "/api/v0/cat" in base else ("GET", f"{base}{cid}")

          def start_check(url: str, max_nodes: int) -> dict:
              r = requests.get(CHECK_HTTP, params={"host": url, "max_nodes": max_nodes},
                               headers={"Accept": "application/json"}, timeout=15)
              r.raise_for_status()
              return r.json()

          def poll_result(request_id: str, timeout_s: float = 25.0, interval_s: float = 0.8) -> dict:
              t0 = time.time()
              while True:
                  r = requests.get(CHECK_RESULT.format(rid=request_id),
                                   headers={"Accept": "application/json"}, timeout=15)
                  r.raise_for_status()
                  data = r.json()
                  if data and all(v is not None for v in data.values()): return data
                  if time.time() - t0 > timeout_s: return data or {}
                  time.sleep(interval_s)

          def summarize(node_results: dict):
              total = ready = ok_nodes = 0
              codes, fastest = [], None
              for _, arr in node_results.items():
                  total += 1
                  if arr is None or not arr: continue
                  ready += 1
                  entry = arr[0]
                  try: success = int(entry[0])
                  except: success = 0
                  try: t = float(entry[1]) if entry[1] is not None else None
                  except: t = None
                  code = str(entry[3]) if len(entry) > 3 and entry[3] is not None else ""
                  if code: codes.append(code)
                  if code or success == 1:
                      ok_nodes += 1
                      if t is not None: fastest = t if fastest is None else min(fastest, t)
              return {
                  "nodes_total": total, "nodes_ready": ready, "nodes_ok": ok_nodes,
                  "codes": "|".join(sorted(set(codes))),
                  "fastest_s": "" if fastest is None else f"{fastest:.3f}",
                  "exists": 1 if ok_nodes > 0 else 0,
              }

          def main():
              ap = argparse.ArgumentParser()
              ap.add_argument("input")
              ap.add_argument("--cid", default=DEFAULT_CID)
              ap.add_argument("--nodes", type=int, default=5)
              ap.add_argument("--csv", default="reports/results_checkhost.csv")
              ap.add_argument("--sleep", type=float, default=0.6)
              ap.add_argument("--online-list", help="Output file for online gateways")
              args = ap.parse_args()

              urls = parse_urls(args.input)
              print(f"Found {len(urls)} URLs; probing with {args.nodes} nodes each...")
              
              results = []
              with open(args.csv, "w", newline="", encoding="utf-8") as f:
                  w = csv.writer(f)
                  w.writerow(["base","method","test_url","exists","nodes_total","nodes_ready","nodes_ok","codes","fastest_s","permanent_link","request_id"])
                  for base in urls:
                      method, test_url = build_test_url(base, args.cid)
                      summary = {}
                      try:
                          kickoff = start_check(test_url, args.nodes)
                          rid = kickoff.get("request_id", ""); perm = kickoff.get("permanent_link", "")
                          node_results = poll_result(rid)
                          summary = summarize(node_results)
                          w.writerow([base, method, test_url, summary["exists"], summary["nodes_total"], summary["nodes_ready"], summary["nodes_ok"], summary["codes"], summary["fastest_s"], perm, rid])
                          print(f"[{ 'ONLINE' if summary.get('exists') else 'OFFLINE' }] {base} ok={summary.get('nodes_ok',0)}/{summary.get('nodes_total',0)} codes={summary.get('codes','')}")
                      except Exception as e:
                          summary = {"exists": 0}
                          w.writerow([base, method, test_url, 0, "", "", "", "", "", "", ""])
                          print(f"[ERROR] {base} -> {e}")
                      results.append((base, summary))
                      time.sleep(args.sleep)
              
              if args.online_list:
                  online_gateways = [base for base, summary in results if summary.get("exists") == 1]
                  with open(args.online_list, "w", encoding="utf-8") as f:
                      for url in online_gateways:
                          f.write(f"{url}\n")
                  print(f"Saved {len(online_gateways)} online gateways to {args.online_list}")

              print(f"Done. CSV saved to: {args.csv}")

          if __name__ == "__main__":
              main()
          '''
          pathlib.Path("remote_probe_checkhost.py").write_text(textwrap.dedent(code), encoding="utf-8")
          pathlib.Path("remote_probe_checkhost.py").chmod(0o755)
          PY

      - name: Prepare initial gateway list for probe
        shell: bash
        run: |
          set -euo pipefail
          # This step ensures a gateway list exists for the probe to use.
          # It prioritizes gateways.txt, then an existing gateways.js, then falls back.
          if [[ -f "gateways.txt" ]] && [[ -s "gateways.txt" ]]; then
            echo "Using gateways.txt for probe."
            exit 0
          elif [[ -f "gateways.js" ]] && grep -qE 'downloadGateways\s*=\s*\[' gateways.js; then
            echo "Using existing gateways.js for probe."
            exit 0
          else
            echo "Creating fallback gateways.js for probe."
            node -e 'const fs=require("fs");const list=["https://ipfs.io/ipfs/","https://cloudflare-ipfs.com/ipfs/"];const js="const downloadGateways = [\n  "+list.map(u=>JSON.stringify(u)).join(",\n  ")+"\n];\n";fs.writeFileSync("gateways.js",js);'
          fi

      - name: Run probe and generate online list
        id: runprobe
        shell: bash
        env:
          NODES: ${{ github.event.inputs.nodes }}
          CID: ${{ github.event.inputs.cid }}
        run: |
          set -euo pipefail
          # Prefer gateways.txt if it exists, otherwise use gateways.js
          LIST_FILE="gateways.js"
          if [[ -f gateways.txt ]] && [[ -s gateways.txt ]]; then
            LIST_FILE="gateways.txt"
          fi

          mkdir -p reports
          EXTRA_ARGS=""
          if [[ -n "${CID:-}" ]]; then
            EXTRA_ARGS="--cid ${CID}"
          fi
          
          # Run the probe and create online_gateways.txt
          python remote_probe_checkhost.py "$LIST_FILE" --nodes "${NODES:-5}" --csv reports/results_checkhost.csv --online-list online_gateways.txt $EXTRA_ARGS

          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          cp reports/results_checkhost.csv "reports/ipfs_gateways_${TS}.csv"
          cp reports/results_checkhost.csv "reports/latest_ipfs_gateways.csv"

      - name: Build gateways.js from online list
        shell: bash
        run: |
          set -euo pipefail
          # Create a new gateways.js using only the online gateways
          if [[ ! -f "online_gateways.txt" ]]; then
            echo "online_gateways.txt not found, skipping build."
            exit 0
          fi
          node -e '
            const fs = require("fs");
            const arr = fs.readFileSync("online_gateways.txt", "utf8").split(/\r?\n/).map(s => s.trim()).filter(Boolean);
            if (arr.length > 0) {
              const js = "const downloadGateways = [\n  " + arr.map(u => JSON.stringify(u)).join(",\n  ") + "\n];\n";
              fs.writeFileSync("gateways.js", js);
              console.log(`gateways.js rebuilt from online_gateways.txt (count=${arr.length})`);
            } else {
              console.warn("No online gateways found. gateways.js was not updated.");
            }
          '

      - name: Patch index.html with online gateways
        shell: bash
        run: |
          set -euo pipefail
          if [[ ! -f index.html ]]; then
            echo "index.html not found at repo root; skip patch."
            exit 0
          fi

          cat > patch-script.js <<'JAVASCRIPT'
          const fs = require('fs');
          const has = p => fs.existsSync(p);
          const read = p => fs.readFileSync(p,'utf8');

          function buildGwList(){
            // This function now reads the gateways.js that was rebuilt from the online list
            if (has('gateways.js')){
              const txt = read('gateways.js');
              const m = txt.match(/downloadGateways\s*=\s*(\[[\s\S]*?\])\s*;?/m);
              if (m) return m[1];
            }
            // Fallback if gateways.js is missing for some reason
            return "[]";
          }

          let html = read('index.html');
          const gwList = buildGwList();

          const patterns = [
            /\bdownloadGateways\s*=\s*\[[\s\S]*?\]\s*;?/m,
            /\b(?:const|let|var)\s+downloadGateways\s*=\s*\[[\s\S]*?\]\s*;?/m
          ];

          let replaced = false;
          for (const re of patterns){
            if (re.test(html)){
              html = html.replace(re, 'downloadGateways = ' + gwList + ';');
              replaced = true;
              break;
            }
          }

          if (!replaced){
            if (/<\/body>/i.test(html)){
              html = html.replace(/<\/body>/i, '<script>const downloadGateways = ' + gwList + ';<\/script>\\n</body>');
            } else if (/<\/head>/i.test(html)){
              html = html.replace(/<\/head>/i, '<script>const downloadGateways = ' + gwList + ';<\/script>\\n</head>');
            } else {
              html += '\\n<script>const downloadGateways = ' + gwList + ';<\/script>\\n';
            }
            replaced = true;
          }

          fs.writeFileSync('index.html', html);
          console.log(replaced ? 'downloadGateways in index.html updated with online list.' : 'WARN: downloadGateways in index.html was not updated.');
          JAVASCRIPT
          
          node patch-script.js

      - name: Commit changes (CSV + patched files)
        shell: bash
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add reports/*.csv || true
          git add index.html || true
          git add gateways.js || true
          COMMIT_CID="${{ github.event.inputs.cid }}"
          if [[ -z "$COMMIT_CID" ]]; then
            COMMIT_CID="default"
          fi
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "Nothing to commit"
          else
            git commit -m "Probe & patch: Update with online gateways; probe CID=${COMMIT_CID} @ $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            git push
          fi

      - name: Upload artifact (CSV)
        uses: actions/upload-artifact@v4
        with:
          name: ipfs-gateway-probe-${{ github.run_id }}
          path: reports/*.csv
